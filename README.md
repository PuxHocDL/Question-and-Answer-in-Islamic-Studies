# ğŸ•Œ QIAS 2025: Islamic Q&A Shared Task
### Benchmarking LLMs on Islamic Inheritance and General Knowledge

## ğŸ“– Overview

The **QIAS 2025 Shared Task** evaluates Large Language Models (LLMs) on their ability to understand, reason, and answer questions grounded in Islamic knowledge.

It features two subtasks:
- **Subtask 1:** Islamic Inheritance (Ê¿Ilm al-MawÄrÄ«th)
- **Subtask 2:** General Islamic Knowledge

Participants may use prompting, fine-tuning, retrieval-augmented generation (RAG), or any other technique. All datasets are bilingual (Arabic-English) and verified by Islamic scholars.

---

## ğŸ§ª Subtasks

### ğŸ“œ Subtask 1: Islamic Inheritance (MÄ«rÄth)

- Focuses on inheritance-related problems (Ê¿Ilm al-MawÄrÄ«th), requiring precise rule-based reasoning aligned with Islamic jurisprudence.
- Dataset:
  - 20000 MCQs  (training))
  - 1000 MCQs (validation)
  - 1000 MCQs (test)
- Extra data:
  - 3165 IslamWeb fatwas
- Levels: Beginner, Intermediate, Advanced

### ğŸ“š Subtask 2: General Islamic Knowledge

- Covers a broad spectrum of Islamic knowledge, including theology, jurisprudence, biography, and ethics. The difficulty levels reflect increasing depth and complexity.
- Dataset:
  - 1,700 MCQs (700 for validation and 1000 for final test)
- Extra data:
  - Source: major Islamic classical books. The answers to the multiple-choice questions in the validation and test sets are derived from these books.  
- Levels: Beginner, Intermediate, Advanced

---

## âš™ï¸ Installation

### 1. Clone the repository

```bash
git clone https://github.com/PuxHocDL/Question-and-Answer-in-Islamic-Studies.git
cd Question-and-Answer-in-Islamic-Studies
```
### 2. Create a new venv and install requirements
```
### Install dependencies
```bash
pip install -r requirements.txt
``` 
###  ğŸ” API Key Configuration
Before running the project, you need to provide an API key in a .env file.

1. An example file named .env_exemple is provided.

2. You must rename it to .env:
```bash
cp .env_exemple .env
```
3.Open the .env file and add your API key to the appropriate variable: 
```bash
API_KEY=your_api_key_here
```
###  âš™ï¸ Configuration File
We provide an example configuration file:
```bash
cp example.yaml config.yaml
```
You must edit config.yaml to specify:
```bash
input_dir: "/path/to/data"

```
The output directory where prediction results will be stored:
```bash
output_dir: "/path/to/results"

```

## ğŸ§­ Baselines
The current baseline supports several models using few-shot prompting-based inference:


Fanar LLM (API) â€” designed for Arabic tasks 
ğŸ”‘ You can request free API access here: https://api.fanar.qa/docs

Mistral (Groq API) â€” a  open-weight model, accessed via the free Groq API: https://groq.com/

Gemini (API) - accessed via the free Google Gemini API: https://aistudio.google.com/app/apikey


### â–¶ï¸ How to Run the Code
#### Configure the Models to Use

- Open the config.yaml file.
- Enable or disable the LLM models by setting "Y" or "N" under the models section:
```bash
models:
  mistral: "Y"
  fanar_rag: "N"
  gemini: "Y"
```
 Customize the Configuration

- Modify config.yaml to change input/output paths, enable/disable models, or set parameters.
- To add new models, edit scripts/models.py and update the MODEL_FUNCTIONS dictionary in scripts/main.py.
#### Run Predictions from the Notebook

Open the prediction.ipynb notebook in Jupyter or VS Code.

Run the first code cell::
```bash
from scripts.inference import process_csv_file
from scripts.models import get_prediction_mistral, get_prediction_fanar
from scripts.utils import get_filename_suffix, save_mcq_file
from scripts.main import load_config,  predict_from_directory

predict_from_directory(config_path="../config.yaml")

```
This script will:

- Load the Excel files from the input directory.
- Apply the selected models to generate predictions.
- Automatically detect the number of options (4 or 6).
- Save the output files in the output directory (output_dir), appending the appropriate suffix (_subtask1.xlsx or _subtask2.xlsx).
- Format the Excel files for better readability.

#### ğŸ“Š Evaluation
1. Ensure Ground Truth Answers Are Available
Make sure each Excel file in the input directory contains a column named answer with the correct answers.

2. Run Evaluation from the Notebook
After generating predictions, add and run the following cell in the prediction.ipynb notebook:
```bash
from  scripts.evaluation import evaluate

# âœ… Specify your file paths
prediction_dir  =  "../results/prediction/Task1_QCM_Dev_fanar_rag_subtask1_prediction.csv"
reference_dir = "../data/Task1_QCM_Dev.csv"
output_dir = '../results/prediction/output' 

# âœ… Call the evaluation function
accuracy = evaluate(reference_dir, prediction_dir, output_dir)
```


## ğŸ“ˆ Evaluation Metrics

### ğŸ“„ Submission Format

Participants must submit a UTF-8 encoded CSV file, with one row per question.

**File Naming Convention:**

- `subtask1_<team_name>_predictions.csv` for SubTask 1 (Islamic Inheritance Reasoning)
- `subtask2_<team_name>_predictions.csv` for SubTask 2 (Islamic Knowledge Assessment)

### ğŸ“Š Required Columns (exact order)

**SubTask 1:**

| Column Name | Description |
|-------------|-------------|
| `id_question` | Unique identifier for each question |
| `prediction` | Modelâ€™s predicted answer (A, B, C, D, E, or F) |

**SubTask 2:**

| Column Name | Description |
|-------------|-------------|
| `id_question` | Unique identifier for each question |
| `prediction` | Modelâ€™s predicted answer (A, B, C, or D) |

### ğŸ§® Evaluation Metric

Model performance will be evaluated based on **accuracy**:

> The percentage of questions for which the modelâ€™s prediction exactly matches the correct answer.

Once submitted, predictions will be automatically evaluated by the QIAS 2025 organizers, and results will be shared with all participating teams.

---

âœ… **READY TO PARTICIPATE?**

We are excited to see your contributions to Islamic AI benchmarking! ğŸš€
